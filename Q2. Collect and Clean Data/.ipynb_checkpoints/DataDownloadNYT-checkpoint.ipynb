{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Team:<br> Alexander Umale, <br>Vikrant</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Article Download from NYT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Define Required Functions to:\n",
    "##### 1. Search NYT for given Query , API Key and Date Range\n",
    "##### 2. Download Articles and Save them at Disk (~/Data/NYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAPIkey(file='./data/nyt_api.key') :\n",
    "    \"\"\"\n",
    "    Get New York Times API key from a file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        Full designated path of the key file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        String of API key\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file) as fp:\n",
    "            key = fp.read().strip()\n",
    "            return key\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchNYTimes(api_key='', query='', fq='', \n",
    "                   fields='', sort='', begin_date='YYYYMMDD', \n",
    "                   end_date='YYYYMMDD', page=-1,):\n",
    "    \"\"\"\n",
    "    Search New York times for articles through provided API.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    api_key : str\n",
    "        NYtimes API key string\n",
    "    query : str\n",
    "        Search query term. Search is performed on the article body, headline and byline.\n",
    "    fq : str\n",
    "        Filtered search query using standard Lucene syntax.\n",
    "        The filter query can be specified with or without a limiting field: label.\n",
    "        See Filtering Your Search for more information about filtering\n",
    "    begin_date : str\n",
    "        Format: YYYYMMDD\n",
    "        Restricts responses to results with publication dates of the date specified or later.\"\n",
    "    end_date : str\n",
    "        Format: YYYYMMDD\n",
    "        Restricts responses to results with publication dates of the date specified or earlier.\n",
    "    sort : str\n",
    "        By default, search results are sorted by their relevance to the query term (q). Use the sort parameter to sort by pub_date.\n",
    "        Allowed values are:\n",
    "            > newest\n",
    "            > oldest\n",
    "    fields : string\n",
    "        Comma-delimited list of fields (no limit)\n",
    "        Limits the fields returned in your search results. By default (unless you include an fl list in your request), \n",
    "        the following fields are returned: snippet, lead_paragraph, abstract, print_page, blog, source, multimedia, \n",
    "        headline, keywords, pub_date, document_type, news_desk, byline, type_of_material, _id, word_count\n",
    "    page : int\n",
    "        The value of page corresponds to a set of 10 results (it does not indicate the starting number of the result set). \n",
    "        For example, page=0 corresponds to records 0-9. To return records 10-19, set page to 1, not 10.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary representation of json object\n",
    "    \"\"\"\n",
    "    # hardcoded link to article search api json object\n",
    "    api_search_url= 'https://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "\n",
    "    if len(query) < 1:\n",
    "        print('Query string is empty')\n",
    "    \n",
    "    fl_items = ['web_url',\n",
    "    'snippet',\n",
    "    'lead_paragraph',\n",
    "    'abstract',\n",
    "    'print_page',\n",
    "    'blog',\n",
    "    'source',\n",
    "    'multimedia',\n",
    "    'headline',\n",
    "    'keywords',\n",
    "    'pub_date',\n",
    "    'document_type',\n",
    "    'news_desk',\n",
    "    'byline',\n",
    "    'type_of_material',\n",
    "    '_id',\n",
    "    'word_count']\n",
    "    \n",
    "    search_param={'api-key':api_key,\n",
    "                  'q':query }\n",
    "    \n",
    "    if len(fq) > 0 :\n",
    "        search_param['fq'] = fq\n",
    "        \n",
    "    if len(fields) > 0:\n",
    "        if set(fields).issubset(fl_items) :\n",
    "            search_param['fl'] = fields\n",
    "        else:\n",
    "            print('Enter valid field values')\n",
    "            return None\n",
    "    if len(sort) > 0:\n",
    "        if sort == 'newest' | sort == 'oldest':\n",
    "            search_param['sort'] = sort\n",
    "    \n",
    "    if begin_date != 'YYYYMMDD':\n",
    "        if int(begin_date[4:6]) > 0 & int(begin_date[4:6]) <= 12:\n",
    "            if int(begin_date[6:9]) > 0 & int(begin_date[6:9]) <= 31:\n",
    "                search_param['begin_date'] = begin_date\n",
    "                \n",
    "    if end_date != 'YYYYMMDD':\n",
    "        if int(begin_date[4:6]) > 0 & int(begin_date[4:6]) <= 12:\n",
    "            if int(begin_date[6:9]) > 0 & int(begin_date[6:9]) <= 31:\n",
    "                search_param['end_date'] = end_date\n",
    "    \n",
    "    if page >= 0:\n",
    "#        print('page is {}'.format(page))\n",
    "        search_param['page'] = page\n",
    "    \n",
    "    try:\n",
    "#        print('search params: {}'.format(search_param))\n",
    "        resp = requests.get(url=api_search_url,params=search_param)\n",
    "#         print(resp.text)\n",
    "#         print(resp.status_code)\n",
    "        response_json = resp.json()\n",
    "        resp.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    if(response_json != None):\n",
    "        return response_json\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYTapiResponseWrapper:\n",
    "    \"\"\"Python Wrapper class for the json object returned by NYtimes API\"\"\"\n",
    "    def __init__(self, response_json = {}):\n",
    "        \"\"\"\n",
    "        Constructor of Wrapper class\n",
    "        Parameters\n",
    "        ----------\n",
    "        response_json : dict\n",
    "            response json object from API\n",
    "        \"\"\"\n",
    "        if len(response_json.keys()) > 0:\n",
    "            self.status = response_json['status']\n",
    "            self.copyright = response_json['copyright']\n",
    "            self._response = response_json['response']\n",
    "            self._parseResponse(self._response)\n",
    "    \n",
    "    def parseJSON(self, response_json = {} ):\n",
    "        \"\"\"\n",
    "        Parser function to parse json object if Wrapper is not initialized with a json\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        response_json : dict\n",
    "            response json object from API\n",
    "        \"\"\"\n",
    "        self.status = response_json['status']\n",
    "        self.copyright = response_json['copyright']\n",
    "        self._response = response_json['response']\n",
    "        self._parseResponse(self._response)\n",
    "\n",
    "    def _parseResponse(self, response):\n",
    "        self._docs = response['docs']\n",
    "        self._meta = response['meta']\n",
    "        self._parseDocs(self._docs)\n",
    "    \n",
    "    def _parseDocs(self, docs):\n",
    "        self.docs = []\n",
    "        i = 0\n",
    "        for doc_item in docs:\n",
    "#            print(i)\n",
    "            i += 1\n",
    "            self.docs.append(Doc(doc = doc_item))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc:\n",
    "    def __init__(self, doc = {}):\n",
    "        self._id = doc['_id']\n",
    "        self.blog             = doc['blog']\n",
    "        self.document_type    = doc['document_type']\n",
    "        self.headline         = doc['headline']\n",
    "        self.keywords         = doc['keywords']\n",
    "        self.multimedia       = doc['multimedia']\n",
    "        self.score            = doc['score']\n",
    "        self.snippet          = doc['snippet']\n",
    "        self.type_of_material = doc['type_of_material']\n",
    "        self.web_url          = doc['web_url']\n",
    "        self.word_count       = doc['word_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getPageByURL(URL = ''):\n",
    "    try:\n",
    "        resp = requests.get(url=URL)\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        resp.close()\n",
    "        return soup\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveArticleText(headline, textParasSoup, filename):\n",
    "    try:\n",
    "        with open(filename, 'w') as fp:\n",
    "            fp.write(headline)\n",
    "            for para in textParasSoup:\n",
    "                fp.write(para.text)\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticlesInMass(api_key='', query='', fq='', \n",
    "                   fields='', sort='', begin_date='YYYYMMDD', \n",
    "                   end_date='YYYYMMDD', page_count = 1, write_to_file=False, \n",
    "                      filename='./data/Articles/ArticleList.json'):\n",
    "    \"\"\"\n",
    "    Get multiple pageset of articles instead of 1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    page_count : int\n",
    "        number of pageset giving 10 articles for each count, i.e. page_count of i will give i*10 articles\n",
    "    write_to_file : boolean\n",
    "        Write obtained article list to a file\n",
    "    filename : str\n",
    "        Name of file if write_to_file is True\n",
    "    Remainig params are same as searchNYTimes function\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of all the articles obtained from API\n",
    "    \"\"\"\n",
    "    article_list = []\n",
    "    for page in range(0,page_count):\n",
    "        resp = searchNYTimes(api_key=api_key, query=query, fq=fq, fields=fields,sort=sort,\n",
    "                             begin_date=begin_date,end_date=end_date, page=page)\n",
    "        resp_ob = NYTapiResponseWrapper(resp)\n",
    "        if len(resp_ob.docs) <= 0 : \n",
    "            break\n",
    "#         print(resp_ob._meta['offset'])\n",
    "        for doc_item in resp_ob.docs:\n",
    "            article = {'id':doc_item._id,'headline':doc_item.headline['main'], 'url':doc_item.web_url, 'downloaded':'N'}\n",
    "            article_list.append(article)\n",
    "        time.sleep(1)\n",
    "    if write_to_file:\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(article_list, file)\n",
    "    return article_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupByCategories(article_list=[]):\n",
    "    \"\"\"\n",
    "    Group articles into article categories\n",
    "\n",
    "    Category is obtained from the http link in below format:\n",
    "    1(https://www.nytimes.com/2018/05/04/) 2(movies) 3(/sandra-bullock-mindy-kaling-oceans-8.html)\n",
    "\n",
    "    2 gives the category\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    article_list : list\n",
    "        List of Articles \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of articles where key is category and value is list of articles    \n",
    "    \"\"\"\n",
    "    groupedArticles = {}\n",
    "    for article in article_list:\n",
    "        url = article['url']\n",
    "        print(url)\n",
    "        # split the url into 3 parts\n",
    "        # eg. 1(https://www.nytimes.com/2018/05/04/) 2(movies) 3(/sandra-bullock-mindy-kaling-oceans-8.html)\n",
    "        reg_ex = \"([a-zA-Z0-9\\.\\-_/:]*/[0-9]{4}/[0-9]{2}/[0-9]{2}/)([a-zA-Z0-9]+)(/[a-zA-Z\\-\\.]*)\"\n",
    "        if len(re.split(reg_ex, url)) < 3: \n",
    "            continue\n",
    "        category = re.split(reg_ex, url)[2]\n",
    "        if not category in groupedArticles:\n",
    "            groupedArticles[category] = [article]\n",
    "        else: \n",
    "            groupedArticles[category].append(article)\n",
    "    return groupedArticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticleListByCategory(category='business', api_key='', begin_date='YYYYMMDD', \n",
    "                   end_date='YYYYMMDD', page_count = 0):\n",
    "    \"\"\"\n",
    "    Returns article of a specific category\n",
    "    \"\"\"\n",
    "    return getArticlesInMass(api_key=api_key, fq='web_url:*'+category+'*',page_count=page_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadAllArticles(article_list, grouped=False, parentDirectory='./data/NYT-articles/'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Download all the articles in the List\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    article_list : list/dict\n",
    "        A list or dict of articles\n",
    "    grouped : boolean\n",
    "        if grouped is True a dictionary of grouped articles is expceted in article_list\n",
    "        else a list of articles\n",
    "    parentDirectory : str\n",
    "        Path of parent directory where articles files are saved by article id file name\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(article_list) <1:\n",
    "        return None\n",
    "    if not grouped:\n",
    "        if type(article_list) == list:\n",
    "            total_articles_written = _downloadArticles(article_list, directory = parentDirectory)\n",
    "        else:\n",
    "            print('Error: List Expected '+type(article_list).__name__+' found')\n",
    "            return None\n",
    "    else:\n",
    "        if type(article_list) == dict:\n",
    "            total_articles_written = 0\n",
    "            for category in article_list.keys():\n",
    "                articles_written = _downloadArticles(article_list[category], directory=parentDirectory + category+'/')\n",
    "                total_articles_written += articles_written\n",
    "        else: \n",
    "            print('Error: Dict expected '+type(article_list).__name__+' found')\n",
    "            return None\n",
    "    return total_articles_written\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _downloadArticles(article_list=[], directory='./'):\n",
    "    if len(article_list) <1:\n",
    "        return None\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    articles_written = 0\n",
    "    for article in article_list:\n",
    "        article_soup = getPageByURL(URL = article['url']) \n",
    "#         print(article['id'], article['url'])\n",
    "        paras = article_soup.find_all('p')\n",
    "        article_text = ''\n",
    "        for para in paras:\n",
    "            if 'class' in para.attrs: \n",
    "                p_class = ' '.join(para.attrs['class'])\n",
    "                if 'css-1' in p_class and ' e2' in p_class or 'story' in p_class: \n",
    "                    article_text += para.text +'\\n'\n",
    "        try:\n",
    "            with open(directory + article['id'], 'w', encoding='utf-8') as file:\n",
    "                file.write(article['headline']+ '\\n')\n",
    "                file.write(article_text)\n",
    "                time.sleep(1)\n",
    "                articles_written += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            # future: write the article list to file once done or occurance of an excecption\n",
    "            pass\n",
    "    return articles_written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Download articles for categories \"Business\", \"Sports\", \"Politics\" and \"Art\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n"
     ]
    }
   ],
   "source": [
    "#article_business = getArticleListByCategory(category='business', api_key='9afbcc4f25e14cafbaad60cf990d0538', begin_date='20180101', \n",
    " #                  end_date='20180505', page_count = 101)\n",
    "business = getArticleListByCategory(category='business', api_key='e4dc65f5fd794792895d12c9554efe04', begin_date='20180101', \n",
    "                   end_date='20180509', page_count = 10) \n",
    " \n",
    "art = getArticleListByCategory(category='art', api_key='e4dc65f5fd794792895d12c9554efe04', begin_date='20180101', \n",
    "                   end_date='20180509', page_count = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n",
      "Query string is empty\n"
     ]
    }
   ],
   "source": [
    "politics = getArticleListByCategory(category='politics', api_key='e4dc65f5fd794792895d12c9554efe04', begin_date='20180101', \n",
    "                   end_date='20180509', page_count = 10)  \n",
    "sports = getArticleListByCategory(category='sports', api_key='e4dc65f5fd794792895d12c9554efe04', begin_date='20180101', \n",
    "                   end_date='20180509', page_count = 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "artCatFiles = downloadAllArticles(musicGrp, grouped=False, parentDirectory='./data/NYT-articles/')musicCatFiles = downloadAllArticles(musicGrp, grouped=True, parentDirectory='./data/NYT-articles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessCatFiles = downloadAllArticles(businessGrp, grouped=False, parentDirectory='./data/NYT-articles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sportsCatFiles = downloadAllArticles(sportsGrp, grouped=False, parentDirectory='./data/NYT-articles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicsCatFiles = downloadAllArticles(politics, grouped=False, parentDirectory='./data/NYT-articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Follow up code: \n",
    "#####  Text Classification Using PySpark.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
